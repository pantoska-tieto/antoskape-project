name: Testing

on:
  workflow_call:
    inputs:
      docker_image:
        description: Docker image to use in tests
        required: true
        type: string
      runner_label:
        description: Label of the runner specified in Build workflow
        required: true
        type: string
      board_target:
        description: Target board for the build
        required: true
        type: string
      test_hardware:
        description: Tests target
        required: true
        type: string
      test_tag:
        description: Test TAGs to filter tests
        required: false
        type: string
      pytest_args:
        description: Arguments to the pytest subprocess (extend YAML config)
        required: false
        type: string
      test_scenario:
        description: Test suite scenario to run
        required: false
        type: string
      tests_target:
        description: Tests scope to run
        required: true
        default: app/repo/shell
        type: string
      integration_tests:
        description: Condition to run integration tests only
        required: false
        type: string

env:
  # Pass all secrets as json to jobs
  SECRETS_JSON: ${{ toJSON(secrets) }}

jobs:
  testing:
    name: Start testing
    runs-on: ${{ inputs.runner_label }}
    outputs:
      test_type: ${{ inputs.tests_target }}
      label: ${{ inputs.runner_label }}
    defaults:
      run:
        working-directory: customer-application
    container:
      image: ${{ inputs.docker_image }}
      options: >
        --device=/dev/ttyUSB0
        --privileged
        -v /var/run/dbus:/var/run/dbus 
        -v /run/dbus:/run/dbus 
        -v /dev:/dev
        -v /etc/udev/rules.d:/etc/udev/rules.d
        -v /sys/class/bluetooth:/sys/class/bluetooth
        -v /var/lib/bluetooth:/var/lib/bluetooth
        -e DBUS_SESSION_BUS_ADDRESS=unix:path=/run/dbus/system_bus_socket
    env:
      CMAKE_PREFIX_PATH: /opt/toolchains
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          path: customer-application
      
      # Initiate Zephyr environment
      - name: Initialize
        shell: bash
        run: |
            echo "Checking out Zephyr env..."
            pwd
            echo "Current github workspace:"
            ls -la
            echo "Parent github folder"
            ls -la ..
            rm -rf ../.west
            west init -l .
            west update -o=--depth=1 -n
            west blobs fetch hal_espressif
            
      # Install lib extras for testing
      - name: Install lib extras
        shell: bash
        run: |
            sudo apt-get update
            sudo apt-get install -y iperf
            sudo apt-get install -y sysbench fio
            sudo apt-get install -y curl
            sudo apt-get install -y udev            
            sudo apt-get install -y python3-dev
            sudo apt-get install -y libgpiod-dev gpiod
            sudo apt-get install -y golang-go
            sudo apt-get install -y bluez bluez-tools

      # Install Python extras for testing
      - name: Install Python extra packages
        shell: bash
        run: |
            pip install -r requirements-extras.txt

      # Setup for tools
      - name: Setup for test tools
        shell: bash
        run: |
            go install github.com/apache/mynewt-mcumgr-cli/mcumgr@latest
            echo "Home dir:" $HOME
            sudo ls -la $HOME/go/bin
            sudo setcap cap_net_raw,cap_net_admin+eip $HOME/go/bin/mcumgr
            getcap $HOME/go/bin/mcumgr || echo "No mcumgr capabilities set"
            sudo rm -f /run/dbus/pid
            sudo dbus-daemon --system --fork
            sudo bluetoothd --nodetach & 
            hciconfig
            btmgmt info

      #  Review of variables from Build workflow
      - name: Show variables from caller workflow
        shell: bash
        run: |
            echo "Variables from caller workflow:"
            echo "docker_image: ${{ inputs.docker_image }}"
            echo "runner_label: ${{ inputs.runner_label }}"
            echo "integration_tests: ${{ inputs.integration_tests }}"
            echo "board_target: ${{ inputs.board_target }}"
            echo "tests_target: ${{ inputs.tests_target }}"
            echo "test_scenario: ${{ inputs.test_scenario }}"
            echo "test_tag: ${{ inputs.test_tag }}"
            echo "pytest_args: ${{ inputs.pytest_args }}"
            echo "test_hardware: ${{ inputs.test_hardware }}"
            
      # Generate list of all tests in application in txt-format
      - name: Create list of all application tests
        shell: bash
        run: python .github/scripts/tests_yaml_parser.py

      # Generate tests paths to run with twister command
      - name: Create test list with tests scope
        shell: bash
        # No test scenario is filtered
        if : ${{ inputs.test_scenario == 'N/A' || '' }} 
        run: python .github/scripts/get_tests_list.py --tests_scope ${{ inputs.tests_target }}

      - name: Create test list with test scenario
        shell: bash
        # Specific test scenario is filtered
        if : ${{ inputs.test_scenario != 'N/A' || '' }} 
        run: python .github/scripts/get_tests_list.py --tests_scenario ${{ inputs.tests_target }} 
        
      - name: Create test list with robot tests
        shell: bash
        # Specific test scenario is filtered
        if : ${{ inputs.tests_target == 'app/robot' }} 
        run: python .github/scripts/get_tests_list.py --tests_robot ${{ inputs.tests_target }} 

      # Put list of testcases into output variable for shell script usage
      - name: Get tests list
        shell: python
        id: py-script
        run: |
            import os
            import re
            regex = re.compile('.*_tests.txt')
            for dir in os.listdir('.'):
                if regex.match(dir):
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f'test_list={os.path.join(dir)}\n')

      # Pass secrets for tests into json file
      - name: Create json file with secrets variables
        shell: bash
        run: |
            echo $SECRETS_JSON > vars.json
            cat vars.json

      # Auto mapping of USB ports to udev rules
      - name: Auto mapping of USB ports to udev rules
        shell: bash
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        run: |
            sudo chmod +x .github/scripts/udev_mapping_serial_ports.sh
            .github/scripts/udev_mapping_serial_ports.sh ${{ inputs.board_target }}

      # Run Twister tests (read serial ports from udev_mapping.txt file created in previous step)
      - name: Run Twister tests
        shell: bash
        run: |
            python .github/scripts/run_tests.py --platform ${{ inputs.board_target }} \
            --device_serial udev_mapping.txt \
            --tag ${{ inputs.test_tag }} \
            --pytest_args ${{ inputs.pytest_args }} \
            --scenario ${{ inputs.test_scenario }} \
            --test_list ${{steps.py-script.outputs.test_list}} \
            --target ${{ inputs.tests_target }} \
            --integration_tests ${{ inputs.integration_tests }} \
            --test_hardware ${{ inputs.test_hardware }}

      # Generate test run metadata json file
      - name: Create metadata json file
        shell: bash
        run: python .github/scripts/tests_metadata_parser.py --artifactory_user ${{ secrets.ARTIFACTORY_USER }} --artifactory_pwd ${{ secrets.ARTIFACTORY_PWD }}

      # Generate test report in html-format
      - name: Create HTML test report
        shell: bash
        run: python .github/scripts/meta_report.py --run_id ${{ github.run_id }}

      # Archive all test artifacts except Robot tests
      - name: Single test artifacts for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target != 'app/robot' }} 
        with:
          name: summary_test_results_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister-out*/*
          include-hidden-files: true
          if-no-files-found: error
          retention-days: 5

      # Upload json files for test statistics to artifacts
      - name: Archive test results twister.json files for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: twister_out_json_files_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister-out*/twister.json
          retention-days: 5

      # Upload metadata json file to artifacts
      - name: Upload metadata json file to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: metadata_file_artifacts
          # Relative path to project root. 
          path: |
            customer-application/metadata.json
            customer-application/metadata.json.bckp
          retention-days: 5

      # Upload list of available application tests to artifacts
      - name: Upload list of all application tests to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: application_tests_lists_artifacts
          # Relative path to project root. 
          path: |
            customer-application/zephyr_tests_list.txt
            customer-application/tests_list.json
          retention-days: 5

      # Upload HTML test report
      - name: Upload HTML test report to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: application_html_test_report_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister_test_report.html
          retention-days: 5

      # Archive robot tests artifacts
      - name: Archive test summary artifacts for Robot-tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target == 'app/robot' }} 
        with:
          name: summary_test_results_artifacts
          # Relative path to project root. 
          path: |
            customer-application/pabot_results/*
          if-no-files-found: error
          retention-days: 5
      
      # Prepare test xml report folder
      - name: Prepare CI report summary for non-Robot tests
        shell: bash
        if : ${{ inputs.tests_target != 'app/robot' }} 
        run: |
            rm -rf test-summary
            sudo mkdir -p test-summary

      # Collect all test output xml reports into one folder
      - name: Copy files to CI report summary
        shell: python
        if : ${{ inputs.tests_target != 'app/robot' }} 
        run: |
            import os
            import re
            import shutil
            regex = re.compile('twister-out.*')
            for dir in os.listdir('.'):
              if regex.match(dir):
                print(f"Found: {dir}")
                if os.path.exists(os.path.join(dir, "twister_suite_report.xml")):
                  shutil.copyfile(os.path.join(dir, "twister_suite_report.xml"), \
                  os.path.join("test-summary", \
                  f"twister_suite_report{dir.split('-out')[1]}.xml"))
                else:
                  print(f"File twister_suite_report.xml not found in {dir}")

      # Archive test output xml reports
      - name: Archive test results twister_suite_report.xml files for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target != 'app/robot' }} 
        with:
          name: twister_out_xml_files_artifacts
          # Relative path to project root. 
          path: |
            customer-application/test-summary
          if-no-files-found: error
          retention-days: 5

      # Archive udev rules file with serial port symlinks
      - name: Archive udev rules file
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        with:
          name: udev_rules_file_artifacts
          # Relative path to project root. 
          path: |
            customer-application/udev_mapping.txt
          if-no-files-found: error
          retention-days: 5

      # Send test report by email
      - name: Send test report email
        shell: bash
        run: python .github/scripts/test_report_email.py --server_pwd "${{ secrets.EMAIL_SERVER_PASSWD }}"

      # Teardown: remove udev file from host machine with serial ports mapping
      - name: Remove udev file with serial ports mapping
        shell: bash
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        run: |
            sudo chmod +x .github/scripts/remove_udev_mapping_serial_ports.sh
            .github/scripts/remove_udev_mapping_serial_ports.sh

  publish_test_summaries:
    name: Publish Test Summaries
    if: ${{ needs.testing.outputs.test_type != 'app/robot' }}
    needs: testing
    uses: ./.github/workflows/reports-summary-publish.yml
    with:
      runner: ${{ needs.testing.outputs.label }}
