name: Testing

on:
  workflow_call:
    inputs:
      docker_image:
        description: Docker image to use in tests
        required: true
        type: string
      runner_label:
        description: Label of the runner specified in Build workflow
        required: true
        type: string
      board_target:
        description: Target board for the build
        required: true
        type: string
      test_hardware:
        description: Tests target
        required: true
        type: string
      test_tag:
        description: Test TAGs to filter tests
        required: false
        type: string
      pytest_args:
        description: Arguments to the pytest subprocess (extend YAML config)
        required: false
        type: string
      test_scenario:
        description: Test suite scenario to run
        required: false
        type: string
      tests_target:
        description: Tests scope to run
        required: true
        default: app/repo/shell
        type: string
      integration_tests:
        description: Condition to run integration tests only
        required: false
        type: string

env:
  # Pass all secrets as json to jobs
  SECRETS_JSON: ${{ toJSON(secrets) }}

jobs:
  testing:
    name: Start testing
    runs-on: ${{ inputs.runner_label }}
    outputs:
      test_type: ${{ inputs.tests_target }}
      label: ${{ inputs.runner_label }}
    defaults:
      run:
        working-directory: customer-application
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          path: customer-application
      
      # Check docker container is existing and available
      - name: Check Docker container is available
        shell: bash
        run: |
            echo "Check docker container is available ..."
            if ! docker ps --filter "name=zephyr-env" --format "{{.Names}}" | grep -q zephyr-env; then \
              echo "[Error] Docker container not found! Please check the Docker image is available."
              exit 1
            fi

      # Install lib extras for testing
      - name: Install lib extras
        shell: bash
        run: |
            docker exec -u root zephyr-env bash -c \
            "apt-get update && apt-get install -y \
            iperf sysbench fio curl udev python3-dev libgpiod-dev gpiod golang-go \
            bluez bluez-tools libbluetooth-dev libdbus-1-dev dbus"

      # Setup Bluetooth stack
      - name: Setup for BT stack
        shell: bash
        run: |
            docker exec -u root zephyr-env \
            bash -c "chmod +x .github/scripts/bluetooth_setup.sh && \
            .github/scripts/bluetooth_setup.sh"

      #  Review of variables from Build workflow
      - name: Show variables from caller workflow
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c 'echo "Variables from caller workflow:"
            echo "docker_image: ${{ inputs.docker_image }}"
            echo "runner_label: ${{ inputs.runner_label }}"
            echo "integration_tests: ${{ inputs.integration_tests }}"
            echo "board_target: ${{ inputs.board_target }}"
            echo "tests_target: ${{ inputs.tests_target }}"
            echo "test_scenario: ${{ inputs.test_scenario }}"
            echo "test_tag: ${{ inputs.test_tag }}"
            echo "pytest_args: ${{ inputs.pytest_args }}"
            echo "test_hardware: ${{ inputs.test_hardware }}"'
            
      # Generate list of all tests in application in txt-format
      - name: Create list of all application tests
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/tests_yaml_parser.py"

      # Generate tests paths to run with twister command into repo_tests.txt
      - name: Create test list with tests scope
        shell: bash
        # No test scenario is filtered
        if : ${{ inputs.test_scenario == 'N/A' || '' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/get_tests_list.py --tests_scope ${{ inputs.tests_target }}"

      - name: Create test list with test scenario
        shell: bash
        # Specific test scenario is filtered
        if : ${{ inputs.test_scenario != 'N/A' || '' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/get_tests_list.py --tests_scenario ${{ inputs.tests_target }}"
        
      - name: Create test list with robot tests
        shell: bash
        # Specific test scenario is filtered
        if : ${{ inputs.tests_target == 'app/robot' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/get_tests_list.py --tests_robot ${{ inputs.tests_target }}"

      # Pass secrets for tests into json file
      - name: Create json file with secrets variables
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) -e SECRETS_JSON="${SECRETS_JSON}" zephyr-env \
            bash -c 'echo "$SECRETS_JSON" > vars.json && \
            cat vars.json'

      # Auto mapping of USB ports to udev rules
      - name: Auto mapping of USB ports to udev rules
        shell: bash
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        run: |
            docker exec -u root zephyr-env \
            bash -c "chmod +x .github/scripts/udev_mapping_serial_ports.sh && \
            .github/scripts/udev_mapping_serial_ports.sh ${{ inputs.board_target }}"

      # Run Twister tests (read serial ports from udev_mapping.txt file created in previous step)
      - name: Run Twister tests
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/run_tests.py --platform ${{ inputs.board_target }} \
            --device_serial udev_mapping.txt \
            --tag ${{ inputs.test_tag }} \
            --pytest_args ${{ inputs.pytest_args }} \
            --scenario ${{ inputs.test_scenario }} \
            --test_list repo_tests.txt \
            --target ${{ inputs.tests_target }} \
            --integration_tests ${{ inputs.integration_tests }} \
            --test_hardware ${{ inputs.test_hardware }}"

      # Generate test run metadata json file
      - name: Create metadata json file
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/tests_metadata_parser.py \
            --artifactory_user ${{ secrets.ARTIFACTORY_USER }} --artifactory_pwd ${{ secrets.ARTIFACTORY_PWD }}"

      # Generate test report in html-format
      - name: Create HTML test report
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/meta_report.py --run_id ${{ github.run_id }}"

      # Archive all test artifacts except Robot tests
      - name: Single test artifacts for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target != 'app/robot' }} 
        with:
          name: summary_test_results_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister-out*/*
          include-hidden-files: true
          if-no-files-found: error
          retention-days: 5

      # Upload json files for test statistics to artifacts
      - name: Archive test results twister.json files for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: twister_out_json_files_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister-out*/twister.json
          retention-days: 5

      # Upload metadata json file to artifacts
      - name: Upload metadata json file to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: metadata_file_artifacts
          # Relative path to project root. 
          path: |
            customer-application/metadata.json
            customer-application/metadata.json.bckp
          retention-days: 5

      # Upload list of available application tests to artifacts
      - name: Upload list of all application tests to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: application_tests_lists_artifacts
          # Relative path to project root. 
          path: |
            customer-application/zephyr_tests_list.txt
            customer-application/tests_list.json
          retention-days: 5

      # Upload HTML test report
      - name: Upload HTML test report to artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: application_html_test_report_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister_test_report.html
          retention-days: 5

      # Archive robot tests artifacts
      - name: Archive test summary artifacts for Robot-tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target == 'app/robot' }} 
        with:
          name: summary_test_results_artifacts
          # Relative path to project root. 
          path: |
            customer-application/pabot_results/*
          if-no-files-found: error
          retention-days: 5
      
      # Prepare test xml report folder
      - name: Prepare CI report summary for non-Robot tests
        shell: bash
        if : ${{ inputs.tests_target != 'app/robot' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "rm -rf test-summary && \
            mkdir -p test-summary"

      # Collect all test output xml reports into one folder
      - name: Copy files to CI report summary
        shell: bash
        if : ${{ inputs.tests_target != 'app/robot' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/prepare_ci_report_files.py"

      # Archive test output xml reports
      - name: Archive test results twister_suite_report.xml files for non-Robot tests
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ inputs.tests_target != 'app/robot' }} 
        with:
          name: twister_out_xml_files_artifacts
          # Relative path to project root. 
          path: |
            customer-application/test-summary
          if-no-files-found: error
          retention-days: 5

      # Archive udev rules file with serial port symlinks
      - name: Archive udev rules file
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        with:
          name: udev_rules_file_artifacts
          # Relative path to project root. 
          path: |
            customer-application/udev_mapping.txt
          if-no-files-found: error
          retention-days: 5

      # Send test report by email
      - name: Send test report email
        shell: bash
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "python .github/scripts/test_report_email.py --server_pwd '${{ secrets.EMAIL_SERVER_PASSWD }}'"

      # Teardown: remove udev file from host machine with serial ports mapping
      - name: Remove udev file with serial ports mapping
        shell: bash
        if : ${{ inputs.test_hardware == 'Real_hardware' }} 
        run: |
            docker exec -u $(id -u):$(id -g) zephyr-env \
            bash -c "chmod +x .github/scripts/remove_udev_mapping_serial_ports.sh
            .github/scripts/remove_udev_mapping_serial_ports.sh"

  publish_test_summaries:
    name: Publish Test Summaries
    if: ${{ needs.testing.outputs.test_type != 'app/robot' }}
    needs: testing
    uses: ./.github/workflows/reports-summary-publish.yml
    with:
      runner: ${{ needs.testing.outputs.label }}
