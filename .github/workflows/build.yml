name: Build

on:
  push:
    branches:
      - "*"
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      runner_label:
        description: Label of the runner to execute the job
        required: true
        default: raspberrypi5-production
        type: string
      board_target:
        description: Target board for the build
        required: false
        default: esp32s3_devkitc/esp32s3/procpu
        type: string
      device_serial:
        description: Serial device for connecting target board
        required: false
        default: /dev/ttyUSB0
        type: string
      test_tag:
        description: Test TAGs to filter tests
        required: false
        default: ""
        type: string
      test_pattern:
        description: Pattern to filter specific tests (regex)
        required: false
        default: ""
        type: string
      pytest_args:
        description: Arguments to the pytest subprocess (extend YAML config)
        required: false
        default: ""
        type: string
      test_scenario:
        description: Test suite scenario to run
        required: false
        default: ""
        type: string
      tests_target:
        description: Tests scope to run
        required: false
        default: app/repo
        type: choice
        # Options with a space, parentheses don't work!
        options:
          - "app/repo"
          - "app/integration"
          - "app/unit"
          - "app_all_tests"
          - "zephyr_all_tests"
    
env:
  RUNNER_LABEL: raspberrypi5-production
  BOARD_TARGET: esp32s3_devkitc/esp32s3/procpu
  DEVICE_SERIAL: /dev/ttyUSB0
  TESTS_TARGET: "app/repo"
  TEST_TAG: N/A
  TEST_PATTERN: N/A
  PYTEST_ARGS: N/A
  TEST_SCENARIO: N/A
  # Pass all secrets as json to pytest
  SECRETS_JSON: ${{ toJSON(secrets) }}

jobs:
  process-runner:
    name: Get workflows runner 
    runs-on: ${{ github.event.inputs.runner_label }}
    outputs:
      label: ${{ steps.runner-label.outputs.label }}
      container: ${{ steps.set-container-arch.outputs.container }}      
    # Set runner if no workflow_dispatch value is invoked  
    steps:     
      - name: Set container based on runner architecture
        id: set-container-arch
        shell: bash
        run: |
            echo "Detected runner architecture: $RUNNER_ARCH"
            if [[ $RUNNER_ARCH == 'ARM64' ]]; then
            echo "Set container name to fit the runner arch: zephyrprojectrtos/ci:v0.28.4-arm64"
            echo "container=zephyrprojectrtos/ci:v0.28.4-arm64" >> $GITHUB_OUTPUT
            fi
            if [[ $RUNNER_ARCH == 'X64' ]]; then
            echo "Set container name to fit the runner arch: zephyrprojectrtos/ci:v0.28.4-amd64"
            echo "container=zephyrprojectrtos/ci:v0.28.4-amd64" >> $GITHUB_OUTPUT
            fi

      - name: Set runner label
        id: runner-label
        run: echo "label=${INPUT_RUNNER_LABEL:-$RUNNER_LABEL}" >> $GITHUB_OUTPUT
        env:
          INPUT_RUNNER_LABEL: ${{ github.event.inputs.runner_label }}

  build:
    name: Build and Test
    needs: process-runner
    strategy:
      max-parallel: 1
      fail-fast: false
      matrix:
        sample: [blinky]
        target: [esp32s3_devkitc/esp32s3/procpu]
    runs-on: ${{ needs.process-runner.outputs.label }}
    defaults:
      run:
        working-directory: customer-application
    container:
      image: ${{ needs.process-runner.outputs.container }}
      options: --device=/dev/ttyUSB0
    env:
      CMAKE_PREFIX_PATH: /opt/toolchains
    steps:
      - name: Start workflow-metrics action
        uses: kittychiu/workflow-metrics@v0.4.7
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OWNER_NAME: "pantoska-tieto"
          REPO_NAME: "antoskape-project"
          START_DATE: "2025-09-05"

      - name: Checkout
        uses: actions/checkout@v4
        with:
          path: customer-application

      - name: Set build env variables
        run: |
            echo "RUNNER_LABEL=${INPUT_RUNNER_LABEL:-$RUNNER_LABEL}" >> $GITHUB_ENV
            echo "BOARD_TARGET=${INPUT_BOARD_TARGET:-$BOARD_TARGET}" >> $GITHUB_ENV
            echo "DEVICE_SERIAL=${INPUT_DEVICE_SERIAL:-$DEVICE_SERIAL}" >> $GITHUB_ENV
            echo "TESTS_TARGET=${INPUT_TESTS_TARGET:-$TESTS_TARGET}" >> $GITHUB_ENV
            echo "TEST_TAG=${INPUT_TEST_TAG:-$TEST_TAG}" >> $GITHUB_ENV
            echo "TEST_PATTERN=${INPUT_TEST_PATTERN:-$TEST_PATTERN}" >> $GITHUB_ENV
            echo "PYTEST_ARGS=${INPUT_PYTEST_ARGS:-$PYTEST_ARGS}" >> $GITHUB_ENV
            echo "TEST_SCENARIO=${INPUT_TEST_SCENARIO:-$TEST_SCENARIO}" >> $GITHUB_ENV
        env:
          INPUT_RUNNER_LABEL: ${{ github.event.inputs.runner_label }}
          INPUT_BOARD_TARGET: ${{ github.event.inputs.board_target }}
          INPUT_DEVICE_SERIAL: ${{ github.event.inputs.device_serial }}
          INPUT_TESTS_TARGET: ${{ github.event.inputs.tests_target }}
          INPUT_TEST_TAG: ${{ github.event.inputs.test_tag }}
          INPUT_TEST_PATTERN: ${{ github.event.inputs.test_pattern }}
          INPUT_PYTEST_ARGS: ${{ github.event.inputs.pytest_args }}
          INPUT_TEST_SCENARIO: ${{ github.event.inputs.test_scenario }}
      
      # Initiate Zephyr environment
      - name: Initialize
        shell: bash
        run: |
            echo "Checking out Zephyr env..."
            pwd
            echo "Current github workspace:"
            ls -la
            echo "Parent github folder"
            ls -la ..
            rm -rf ../.west
            west init -l .
            west update -o=--depth=1 -n
            west blobs fetch hal_espressif
            
      # Install lib extras for testing
      - name: Install lib extras
        shell: bash
        run: |
            sudo apt-get update
            sudo apt-get install -y iperf
            sudo apt-get install -y sysbench fio

      # Install Python extras for testing
      - name: Install Python extra packages
        shell: bash
        run: |
            pip install -r requirements-extras.txt

      - name: CPU Benchmark
        run: |
          echo "### CPU Benchmark ###" | tee -a workflow_benchmark.txt
          sysbench cpu --threads=$(nproc) run | tee -a workflow_benchmark.txt

      - name: Memory Benchmark
        run: |
          echo "### Memory Benchmark ###" | tee -a workflow_benchmark.txt
          sysbench memory --threads=$(nproc) run | tee -a workflow_benchmark.txt

      - name: Disk I/O Benchmark
        run: |
          echo "### Disk I/O Benchmark ###" | tee -a workflow_benchmark.txt
          fio --name=seqwrite --filename=testfile --size=1G --bs=1M --iodepth=1 --rw=write | tee -a workflow_benchmark.txt
          fio --name=seqread --filename=testfile --size=1G --bs=1M --iodepth=1 --rw=read | tee -a workflow_benchmark.txt
          rm testfile

      - name: Benchmark - gather System Information
        run: |
          echo "### CPU Information ###" | tee -a workflow_benchmark.txt
          lscpu | tee -a workflow_benchmark.txt
          echo "\n### Memory Information ###" | tee -a workflow_benchmark.txt
          free -h | tee -a workflow_benchmark.txt
          echo "\n### Disk Space Usage ###" | tee -a workflow_benchmark.txt
          df -h | tee -a workflow_benchmark.txt
          echo "\n### Operating System Details ###" | tee -a workflow_benchmark.txt
          uname -a | tee -a workflow_benchmark.txt
          echo "\n### CPU Details from /proc/cpuinfo ###" | tee -a workflow_benchmark.txt
          grep 'model name' /proc/cpuinfo | uniq | tee -a workflow_benchmark.txt
          echo "\n### Total Number of CPU Cores ###" | tee -a workflow_benchmark.txt
          nproc | tee -a workflow_benchmark.txt
          echo "\n### Total Number of CPU Threads ###" | tee -a workflow_benchmark.txt
          lscpu | grep "Thread(s) per core" | tee -a workflow_benchmark.txt
          echo "\n### Disk Details ###" | tee -a workflow_benchmark.txt
          lsblk | tee -a workflow_benchmark.txt

      - name: Upload Benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-stats
          path: workflow_benchmark.txt

      # Generate tests paths to run with twister command
      - name: Create test list with tests scope
        shell: bash
        # No test scenario is filtered
        if : ${{ (env.TEST_SCENARIO == 'N/A' || '')  && ( env.TEST_PATTERN == 'N/A' || '') }} 
        run: python .github/scripts/get_tests_list.py --tests_scope ${{ env.TESTS_TARGET }}

      - name: Create test list with test scenario/test pattern
        shell: bash
        # Specific test scenario/pattern is filtered
        if : ${{ (env.TEST_SCENARIO != 'N/A' || '')  || ( env.TEST_PATTERN != 'N/A' || '') }} 
        run: python .github/scripts/get_tests_list.py --tests_scenario_pattern ${{ env.TESTS_TARGET }}      

      # Pass secrets into json file
      - name: Create json file with secrets variables
        shell: bash
        run: |
            echo $SECRETS_JSON > vars.json
            cat vars.json

      # Put list of testcases into output variable for shell script usage
      - name: Get tests list
        shell: python
        id: py-script
        run: |
            import os
            import re
            regex = re.compile('.*_tests.txt')
            for dir in os.listdir('.'):
                if regex.match(dir):
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f'test_list={os.path.join(dir)}\n')

      # Example for custom application build
      - name: Build ${{ matrix.sample }} for ${{ matrix.target }}
        shell: bash
        run: west build -p always -b ${{ matrix.target }} app-esp32/${{ matrix.sample }}
        
      - name: Flash
        shell: bash
        run: west flash

      # Twister tests section
      - name: Twister tests
        shell: bash
        run: |
            python .github/scripts/run_tests.py --platform ${{ env.BOARD_TARGET }} \
            --device_serial ${{ env.DEVICE_SERIAL }} \
            --tag ${{ env.TEST_TAG }} \
            --test_pattern ${{ env.TEST_PATTERN }} \
            --pytest_args ${{ env.PYTEST_ARGS }} \
            --scenario ${{ env.TEST_SCENARIO }} \
            --test_list ${{steps.py-script.outputs.test_list}} \
      # Archive all test artifacts
      - name: Single test artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ always() }}
        with:
          name: single_test_artifacts
          # Relative path to project root. 
          path: |
            customer-application/twister-out*/*
          include-hidden-files: true
          if-no-files-found: error
          retention-days: 5
      
      # Prepare test summary report folder
      - name: Prepare CI report summary
        shell: bash
        if : ${{ always() }}
        run: |
            rm -rf test-summary
            sudo mkdir -p test-summary

      # Collect all test summary reports into one folder
      - name: Copy files to CI report summary
        shell: python
        if : ${{ always() }}
        run: |
            import os
            import re
            import shutil
            regex = re.compile('twister-out.*')
            for dir in os.listdir('.'):
              if regex.match(dir):
                print(f"Found: {dir}")
                shutil.copyfile(os.path.join(dir, "twister_suite_report.xml"), \
                os.path.join("test-summary", \
                f"twister_suite_report{dir.split('-out')[1]}.xml"))

      # Archive test summary report
      - name: Archive test summary artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        if: ${{ always() }}
        with:
          name: summary_test_artifacts
          # Relative path to project root. 
          path: |
            customer-application/test-summary
          if-no-files-found: error
          retention-days: 5

      - name: Stop workflow-metrics action (upload files)
        uses: actions/upload-artifact@v4
        with:
          name: workflow-metrics-stats
          path: |
            workflow-stats.csv
            runs.json

  publish_test_summaries:
    name: Publish Test Summaries
    if: ${{ always() }}
    needs: build
    uses: ./.github/workflows/reports-summary-publish.yml
